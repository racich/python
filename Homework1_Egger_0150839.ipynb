{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ac4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c190fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# These are helpful functions used within the \"main functions\" \n",
    "\n",
    "def find_quater(xxx): \n",
    "    if bool(re.search(r'\\d{4,4}-[0][1-3]-\\d\\d', xxx))==True:\n",
    "        return \"QTR1\"\n",
    "    if bool(re.search(r'\\d{4,4}-[0][4-6]-\\d\\d', xxx))==True:\n",
    "        return \"QTR2\"\n",
    "    if bool(re.search(r'\\d{4,4}-[0][7-9]-\\d\\d', xxx))==True:\n",
    "        return \"QTR3\"\n",
    "    if bool(re.search(r'\\d{4,4}-[1][0-2]-\\d\\d', xxx))==True:\n",
    "        return \"QTR4\"\n",
    "    else: \n",
    "        print(\"The date needs to be of the form YYYY-MM-DD.\")\n",
    "\n",
    "    \n",
    "\n",
    "def add_values_to_dict(sample_dict, type_and_url):\n",
    "    key = [x[0] for x in type_and_url]\n",
    "    value_urls = [x[1] for x in type_and_url]\n",
    "    index = 0\n",
    "                \n",
    "    for single_key in key: \n",
    "        key_adapted = re.findall(r'\\S*\\s*\\S+', single_key)[0]\n",
    "        \n",
    "                    \n",
    "        if key_adapted not in sample_dict: \n",
    "            sample_dict[key_adapted] = list()\n",
    "        \n",
    "        sample_dict[key_adapted].append(str(value_urls[index]))\n",
    "        index = index + 1\n",
    "      \n",
    "    return sample_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Disclaimer: one could make the function more pretty by using functions for parts of the code that are \n",
    "# repetetive like the functions above. However, I ran out of time and couldn't slim down the code anymore. \n",
    "\n",
    "\n",
    "# I tried all variations for edgar_download_htmlfiles that are listed below and they all work (at some point after multiple reruns of the code)\n",
    "# Further, I included the variable number_of_filings and set it to 11 in order to show that the code does work for\n",
    "# more than 10 filings. However, even though I accounted for the access regulations of EDGAR by including (too many) time.sleep(0.11)\n",
    "# in front of every request, I'm still getting access denials somethimes. \n",
    "# Therefore if you want to check the code and you receive an error message or no output at all, please rerun the code and maybe set the\n",
    "# number_of_filing=1 in order to have a better chance to see the code working fully. \n",
    "\n",
    "\n",
    "def edgar_download_htmlfiles(date, filetype=None, number_of_filings=11):\n",
    "    \n",
    "    #the full index is used because not every date in the daily data has a crawler.date.idx file\n",
    "    url_main = \"https://www.sec.gov/Archives/edgar/full-index/\"\n",
    "    \n",
    "    \n",
    "    # here we are looking at cases where the filetype is specified\n",
    "    # here we dont need to sort by form type, since we are considering only one filetype(=form type)\n",
    "    if filetype != None: \n",
    "    \n",
    "        if type(date)== str: \n",
    "            \n",
    "            year = str(date)[:4] #this is already a string\n",
    "            \n",
    "            #the url used to download the index data of the corresponing date \n",
    "            url_crawler = url_main+year+\"/\"+find_quater(date)+\"/crawler.idx\"\n",
    "            time.sleep(0.11) #under 10 requests per second\n",
    "            crawler_download = requests.get(url_crawler)\n",
    "            \n",
    "            time.sleep(0.11)\n",
    "            soup1 = BeautifulSoup(crawler_download.text, \"html.parser\")\n",
    "        \n",
    "            #get list of all urls that contain the searched filetype and date \n",
    "            url_find_htm = re.findall(filetype+r'\\s*\\d*\\s*'+str(date)+r'\\s*(\\bhttp.*\\b)', str(soup1))\n",
    "             \n",
    "             \n",
    "            # interate over the list of urls that have the correct filetype and date\n",
    "            for wanted in url_find_htm[:number_of_filings]:\n",
    "                time.sleep(0.11) \n",
    "                soup_indiv = BeautifulSoup(requests.get(wanted).text, \"html.parser\")  \n",
    "                \n",
    "                url_find = soup_indiv.table.find_all(string = filetype)\n",
    "                \n",
    "                \n",
    "                # important to use parent here (siblings dont work because find returns Navigatable String and not the <td>) \n",
    "                \n",
    "                # some filings use the same Description as for Type (eg. Description = 8-K, Type=8-K), we are find these Descriptions in our search above\n",
    "                # therefore we need to account for that here and consider the second match\n",
    "                if len(url_find) > 1: \n",
    "                    url_find_double = url_find[1]\n",
    "                    td_tag = url_find_double.parent\n",
    "                    prev_a_tag = td_tag.findPrevious('a')\n",
    "                else: \n",
    "                    td_tag = url_find[0].parent\n",
    "                    prev_a_tag = td_tag.findPrevious('a')\n",
    "                \n",
    "                \n",
    "                #not all filings are saved in .htm \n",
    "                #with this procedure one can download all files of the corresponding type regardless of the file format\n",
    "                #eg download old .txt filings and new .hml filings\n",
    "                \n",
    "                url_file = \"https://www.sec.gov\"+prev_a_tag['href']\n",
    "                time.sleep(0.11)\n",
    "                filing = requests.get(url_file)\n",
    "                \n",
    "                file_name = re.search(r'[\\w\\-]*\\.[a-zA-Z]*$', str(prev_a_tag['href']))\n",
    "                \n",
    "                \n",
    "#### DOWNLOADING THE HTML(or other file format) VERSIONS OF THE FILINGS    \n",
    "                #using the same filename as on the web              \n",
    "                open(str(file_name.group()), 'wb').write(filing.content)\n",
    "                \n",
    "                ########## this is purely used to check if everything worked correctly if the output is <Response [200]> and no other error message occured the entire code execution worked as intended\n",
    "                print(filing)\n",
    "                \n",
    "           \n",
    "        # here lists of dates are considered with a specific filetype\n",
    "        if type(date)== list: \n",
    "            # use a for loop to interate over all dates. Same as above for one date\n",
    "            for date_day in date: \n",
    "                \n",
    "                year = date_day[:4] \n",
    "                \n",
    "                 \n",
    "                url_crawler = url_main+year+\"/\"+find_quater(date_day)+\"/crawler.idx\"\n",
    "                time.sleep(0.11)\n",
    "                crawler_download = requests.get(url_crawler)\n",
    "                time.sleep(0.11)\n",
    "                soup1 = BeautifulSoup(crawler_download.text, \"html.parser\")\n",
    "            \n",
    "                 \n",
    "                url_find_htm = re.findall(filetype+r'\\s*\\d*\\s*'+str(date_day)+r'\\s*(\\bhttp.*\\b)', str(soup1))\n",
    "        \n",
    "                \n",
    "                for wanted in url_find_htm[:number_of_filings]:\n",
    "                    time.sleep(0.11) \n",
    "                    soup_indiv = BeautifulSoup(requests.get(wanted).text, \"html.parser\")  \n",
    "                          \n",
    "                    \n",
    "                    url_find = soup_indiv.table.find_all(string = filetype)\n",
    "                    \n",
    "                    if len(url_find) > 1: \n",
    "                        url_find_double = url_find[1]\n",
    "                        td_tag = url_find_double.parent\n",
    "                        prev_a_tag = td_tag.findPrevious('a')\n",
    "                    else: \n",
    "                        td_tag = url_find[0].parent\n",
    "                        prev_a_tag = td_tag.findPrevious('a')\n",
    "                    \n",
    "                    url_file = \"https://www.sec.gov\"+prev_a_tag['href']\n",
    "                    time.sleep(0.11)\n",
    "                    filing = requests.get(url_file)\n",
    "                     \n",
    "                    file_name = re.search(r'[\\w\\-]*\\.[a-zA-Z]*$', str(prev_a_tag['href']))\n",
    "                  \n",
    "                    open(str(file_name.group()), 'wb').write(filing.content)\n",
    "                    \n",
    "                    print(filing)\n",
    "                \n",
    "              \n",
    "    # here we are considering the case where no filetype is specified\n",
    "    elif filetype == None: \n",
    "        # single date\n",
    "        if type(date)== str: \n",
    "            \n",
    "            year = str(date)[:4] \n",
    "            \n",
    "            url_crawler = url_main+year+\"/\"+find_quater(date)+\"/crawler.idx\"\n",
    "            time.sleep(0.11)\n",
    "            crawler_download = requests.get(url_crawler)\n",
    "            time.sleep(0.11)\n",
    "            soup1 = BeautifulSoup(crawler_download.text, \"html.parser\")\n",
    "                \n",
    "            \n",
    "            #SINCE MY PC IS SLOW I USED FINDITER INSTEAD OF FINDALL (too many elements found) AND ONLY LOOK AT THE FIRST FEW FINDS\n",
    "            #THE CODE CAN BE EASILY ADJUSTED TO USE FINDALL (I got on average 10000 matches for any workday, therefore I dont recomend using findall) \n",
    "            # the grouping helps getting a tuple of filetype and url\n",
    "            url_find_htm_with_type = [m.group(1,2) for m, _ in zip(re.finditer(r'\\s*(\\S+\\s?\\S*)\\s*\\d*\\s*'+str(date)+r'\\s*(\\bhttp.*\\b)', str(soup1)), range(number_of_filings))]\n",
    "            \n",
    "            \n",
    "# I'm using a dictionary for the filetype (key) and the urls (values)(multiple urls for the same key).\n",
    "# The dictionary provides the sorted indices. The keys are not in sorted in alphabetical order but after occurance in crawler.idx, if one would like the keys to be ordered alphabetically one could use OrderedDict from collections. \n",
    "            url_dict = {}\n",
    "            add_values_to_dict(url_dict, url_find_htm_with_type)\n",
    "            \n",
    "             \n",
    "            # interate over keys, this is ordered \n",
    "            for wanted_type in url_dict.keys():\n",
    "                \n",
    "                for wanted in url_dict[str(wanted_type)]:\n",
    "                    \n",
    "                    time.sleep(0.11) \n",
    "                    soup_indiv = BeautifulSoup(requests.get(wanted).text, \"html.parser\")  \n",
    "                              \n",
    "                    url_find = soup_indiv.table.find_all(string = wanted_type)\n",
    "                    \n",
    "                    if len(url_find) > 1: \n",
    "                        url_find_double = url_find[1]\n",
    "                        td_tag = url_find_double.parent\n",
    "                        prev_a_tag = td_tag.findPrevious('a')\n",
    "                    else: \n",
    "                        td_tag = url_find[0].parent\n",
    "                        prev_a_tag = td_tag.findPrevious('a')\n",
    "                    \n",
    "                    \n",
    "                    url_file = \"https://www.sec.gov\"+prev_a_tag['href']\n",
    "                    time.sleep(0.11)\n",
    "                    filing = requests.get(url_file)\n",
    "                    \n",
    "                   \n",
    "                    \n",
    "                    file_name = re.search(r'[\\w\\-]*\\.[a-zA-Z]*$', str(prev_a_tag['href']))\n",
    "                    \n",
    "                    open(str(file_name.group()), 'wb').write(filing.content)\n",
    "                    print(filing)\n",
    "                \n",
    "            \n",
    "        # list of days without filetype    \n",
    "        if type(date)== list: \n",
    "            for date_day in date: \n",
    "                                   \n",
    "                year = date_day[:4] \n",
    "                \n",
    "                url_crawler = url_main+year+\"/\"+find_quater(date_day)+\"/crawler.idx\"\n",
    "                time.sleep(0.11)\n",
    "                crawler_download = requests.get(url_crawler)\n",
    "                time.sleep(0.11)\n",
    "                soup1 = BeautifulSoup(crawler_download.text, \"html.parser\")\n",
    "            \n",
    "                \n",
    "                url_find_htm_with_type = [m.group(1,2) for m, _ in zip(re.finditer(r'\\s*(\\S+\\s?\\S*)\\s*\\d*\\s*'+str(date_day)+r'\\s*(\\bhttp.*\\b)', str(soup1)), range(number_of_filings))]\n",
    "                \n",
    "                \n",
    "                url_dict = {}\n",
    "                add_values_to_dict(url_dict, url_find_htm_with_type)\n",
    "                \n",
    "                             \n",
    "                \n",
    "                for wanted_type in url_dict.keys():\n",
    "                    \n",
    "                    for wanted in url_dict[str(wanted_type)]:\n",
    "                        \n",
    "                        time.sleep(0.11) \n",
    "                        soup_indiv = BeautifulSoup(requests.get(wanted).text, \"html.parser\")  \n",
    "                                  \n",
    "                       \n",
    "                        url_find = soup_indiv.table.find_all(string = wanted_type)\n",
    "                         \n",
    "                        if len(url_find) > 1: \n",
    "                            url_find_double = url_find[1]\n",
    "                            td_tag = url_find_double.parent\n",
    "                            prev_a_tag = td_tag.findPrevious('a')\n",
    "                        else: \n",
    "                            td_tag = url_find[0].parent\n",
    "                            prev_a_tag = td_tag.findPrevious('a')\n",
    "                        \n",
    "                        url_file = \"https://www.sec.gov\"+prev_a_tag['href']\n",
    "                        time.sleep(0.11)\n",
    "                        filing = requests.get(url_file)\n",
    "                        \n",
    "                        \n",
    "                        file_name = re.search(r'[\\w\\-]*\\.[a-zA-Z]*$', str(prev_a_tag['href']))\n",
    "                        open(str(file_name.group()), 'wb').write(filing.content)\n",
    "                        print(filing)\n",
    "                    \n",
    "                       \n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def edgar_10Q(date, filetype = \"10-Q\"): \n",
    "    # I mainly copied the code from above in the case where we have a specified filetype and one single date\n",
    "    \n",
    "    url_main = \"https://www.sec.gov/Archives/edgar/full-index/\"\n",
    "    \n",
    "    year = str(date)[:4]  \n",
    "    url_crawler = url_main+year+\"/\"+find_quater(date)+\"/crawler.idx\"\n",
    "    time.sleep(0.11)\n",
    "    crawler_download = requests.get(url_crawler)\n",
    "    time.sleep(0.11)\n",
    "    soup1 = BeautifulSoup(crawler_download.text, \"html.parser\")\n",
    "        \n",
    "    url_find_htm = re.findall(filetype+r'\\s*\\d*\\s*'+str(date)+r'\\s*(\\bhttp.*\\b)', str(soup1))\n",
    "    \n",
    "        \n",
    "    for wanted in url_find_htm[:1]:\n",
    "        \n",
    "         \n",
    "        time.sleep(0.11) \n",
    "        soup_indiv = BeautifulSoup(requests.get(wanted).text, \"html.parser\")  \n",
    "         \n",
    "        \n",
    "        url_find = soup_indiv.table.find_all(string = filetype)\n",
    "                \n",
    "        if len(url_find) > 1: \n",
    "            url_find_double = url_find[1]\n",
    "            td_tag = url_find_double.parent\n",
    "            prev_a_tag = td_tag.findPrevious('a')\n",
    "        else: \n",
    "            td_tag = url_find[0].parent\n",
    "            prev_a_tag = td_tag.findPrevious('a')\n",
    "                \n",
    "        url_file = \"https://www.sec.gov\"+prev_a_tag['href']\n",
    "        \n",
    "        time.sleep(0.11)\n",
    "        filing = requests.get(url_file)\n",
    "        \n",
    "# DOWNLOADING THE FILING        \n",
    "        file_name = re.search(r'[\\w\\-]*\\.[a-zA-Z]*$', str(prev_a_tag['href']))\n",
    "        open(str(file_name.group()), 'wb').write(filing.content)\n",
    "        \n",
    "        \n",
    "        soup_filing = BeautifulSoup(filing.text, \"html.parser\")\n",
    "     \n",
    "        \n",
    "# DELETING TABLES AND IMAGES \n",
    "        for item in soup_filing.find_all([\"table\", \"picture\"]): \n",
    "            item.decompose()\n",
    "        \n",
    "        \n",
    "        # use regex to find all items of the form eg. Item 1. Hallo this is not a real item\n",
    "        # the regex is not ideal since it potentially misses very long \"item descriptions\". However, it was sufficient for all \"item descriptions\" I came across \n",
    "        item_finder = soup_filing.find_all(string = re.findall(r'[i|I][t|T][e|E][m|M]\\s\\d+\\w*\\.\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*\\s*\\w*', str(soup_filing)))\n",
    "        \n",
    "        # create a DataFrame for items and content to be stored\n",
    "        df = pd.DataFrame([range(len(item_finder))])\n",
    "        # items are the column names\n",
    "        df.columns = item_finder\n",
    "        \n",
    "        # as before we need to consider the parent and then find the next but one sibling (since the next sibling doesnt contain works in the sting)\n",
    "        # the content is written in the first row considering the corresponding column\n",
    "        for i in range(len(item_finder)): \n",
    "            tag = item_finder[i].parent\n",
    "            text_tag = tag.findNext('p').findNext('p')\n",
    "            df.loc[0][i] = text_tag.string\n",
    "        \n",
    "        file_name = re.search(r'[\\w\\-]*\\.[a-zA-Z]*$', str(prev_a_tag['href']))\n",
    "        open(str(file_name.group()), 'wb').write(filing.content)\n",
    "        \n",
    "# RETURNING THE DATAFRAME        \n",
    "        return df\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f520441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all of these function calls worked perfectly at least once, at some point in time\n",
    "# edgar_download_htmlfiles(\"2020-02-12\", \"8-K\")\n",
    "# edgar_download_htmlfiles(\"2020-02-12\")\n",
    "\n",
    "# date_list = [\"2020-02-12\", \"2019-03-01\"]\n",
    "# edgar_download_htmlfiles(date_list, \"8-K\")\n",
    "# edgar_download_htmlfiles(date_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62122b95",
   "metadata": {},
   "source": [
    "I haven't redone the executions to the point, where I don't get an error because I already spent nearly 30 hours on this homework and I'm done. Couldn't figure out where I went wrong with the time.sleep(). Ideally the Response is always 200.\n",
    "Response 403 or the AttributeError are caused by EDGAR \"blocking\" me and downloading the document informing me about their policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9764b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-b8de7e4d6696>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0medgar_download_htmlfiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2020-02-12\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"8-K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-1c2ae2668689>\u001b[0m in \u001b[0;36medgar_download_htmlfiles\u001b[1;34m(date, filetype, number_of_filings)\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[0msoup_indiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwanted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0murl_find\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup_indiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiletype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "edgar_download_htmlfiles(\"2020-02-12\", \"8-K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1e659663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "edgar_download_htmlfiles(\"2020-02-12\", None, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f6f2a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [403]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "date_list = [\"2020-02-12\", \"2019-03-01\"]\n",
    "edgar_download_htmlfiles(date_list, \"8-K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e4c537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "date_list = [\"2020-02-12\", \"2019-03-01\"]\n",
    "edgar_download_htmlfiles(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecbc5e8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findNext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-5bf9f716094d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m## indicating that at some point in the code execution EDGAR \"blocked\" me and I downloaded the document informing me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m## about their policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0medgar_10Q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2020-02-11\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-1c2ae2668689>\u001b[0m in \u001b[0;36medgar_10Q\u001b[1;34m(date, filetype)\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_finder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_finder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m             \u001b[0mtext_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindNext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindNext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_tag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findNext'"
     ]
    }
   ],
   "source": [
    "## I couldnt execute this function without an error message, typical errors were: 'NoneType' object has no attribute \n",
    "## 'find_all' or 'NoneType' object has no attribute 'findNext'\n",
    "## indicating that at some point in the code execution EDGAR \"blocked\" me and I downloaded the document informing me \n",
    "## about their policy\n",
    "edgar_10Q(\"2020-02-11\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c9f97",
   "metadata": {},
   "source": [
    "NOTE: I'll give a brief explanation on how to use a different approch for grabbing the url of the filings: \n",
    "One could generate a DataFrame from the table contained in the ...index.html files and than it is fairly easy to extract\n",
    "the name of the filing from the dataframe. Just search for the type of filing and in the same row one finds the name of the filing document. Next the url needs to be adjusted: when looking at ...index.\n",
    "I'll demonstrate it with an example: /Archives/edgar/data/1770787/0001193125-20-036029-index.htm is the ...index.htm url\n",
    "d846921d8k.htm is the filename, and the file url is /Archives/edgar/data/1770787/000119312520036029/d846921d8k.htm\n",
    "notice here that both urls are nearly the same. One has to simply delete the hyphen (-) from the last part of the url, delete \"index.htm\" and replace if with \"/filename.format\"\n",
    "\n",
    "in retrospect I should have done that instead of grabbing the urls like I did above "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
